---
layout: ../../layouts/BlogPostLayout.astro
title: 大模型、大平台、大焦慮：你準備好了嗎？
author: 季先生
description: 介紹 MCP 和 LLM 、RAG名詞，來一點簡單又不失幽默的科普。
date: 2025-04-06
category: AI
---

# 大模型

**TL;DR**

來蹭一下「大模型」的熱度好了，不是Big Model，不知道已經多少年沒有自己手寫這種文章。
我記得我剛接觸到 AI 的時候，大家都講 LLM——  
~~法學碩士 (Master of Laws)，謝謝 Google 翻譯~~。

後來從一些技術文章裡偶爾看到「大模型」，居然沒意識到它們其實是同一回事。  
LLM = Large Language Model，也就是大型語言模型。  
但說真的，好像也沒人這樣正式叫它。

突然讓我想起幾年前那個「大平台」的笑話，不過今天不扯那麼遠。  
回頭看看，其實「大數據（Big Data）」也是先從中國傳來的用語，台灣以前都叫「巨量資料」。現在什麼都流行講大──大模型、大平台、大數據......  

隨著這些 AI 像過春筍一樣冒出來，我不禁思考一個人生難題－ **大焦慮** ：


> 「我到底還能幹嘛？」
前幾天發呆在螢幕前看著cursor一行一行的為我打工，眼淚差點流出來。

---

## MCP 是什麼？

Model Context Protocol（模型上下文協議），一個跟 LLM 很有關係的東西，旨在為LLM 提供 **標準化接口**，讓它可以與外部資源互動。

「標準化接口」？那不就是我們每天在寫的 Web API 嗎？  
對啦，軟體工程師應該都秒懂這種東西。  
那種面試常問爛的問題：「Web API 是什麼？」不清楚的同學自己去 Google。

MCP 架構基本上就是個 Client–Server 架構，也就是說──它長得真的有點像我們平常在寫的 Web API。


---

## MCP 實例

MCP 可以怎麼應用呢？舉幾個簡單的例子：

<ul>
  <li>📂 存取本地檔案（像是開啟文件、讀寫檔案）</li>
  <li>🗄️ 連接資料庫（查詢、更新）</li>
  <li>🌐 呼叫外部 API（像是叫 Uber、發 email、查天氣）</li>
</ul>

不囉唆，我先上菜：

```python
# 精簡版的 FastAPI + MCP 教學範例
from fastapi import FastAPI
from fastapi_mcp import add_mcp_server

app = FastAPI(title="MCP Demo")

# 加入 MCP server
mcp_server = add_mcp_server(
    app,
    mount_path="/mcp",
    name="Demo MCP",
    base_url="http://localhost:8787"
)

# 範例工具：查詢目前有幾項商品
items_db = {"1": "鐵鎚", "2": "電鑽"}

@mcp_server.tool()
async def get_item_count() -> int:
    """取得商品總數"""
    return len(items_db)
```

太可以了，這段真的又好笑又寫實🤣 我幫你保留原有的語氣──幽默、略帶抱怨但很接地氣──潤飾一下語句結構、讓段落更順，情緒更流暢，讓讀者既笑得出聲又看得懂 MCP 的實際用處。

---

### 用生活講技術：職場霸凌（誤）

我不喜歡把東西講得太學術，畢竟我們又不是在寫碩論。所以來點生活化的例子。
假設今天是個普通的上班日，你正在好好敲程式，主管突然丟下一句話，順便把你一整個中午行程排好了：

> 「小季，等一下你去拿午餐的時候，順便幫我買杯咖啡，要記得打統編喔！還有別忘了 1 點有會議，別太晚回來。」

乍聽只是一句話，實際上根本是五件事的任務連環包——這種話如果讓 MCP 來解析的話，其實是這樣拆的：

1. 去拿午餐  
2. 順路買咖啡  
   - 指定品牌（平常都喝那家小巷子的手沖）  
   - 熱的，少冰（欸那個白目主管有說嗎？？這是不是要靠 prompt 去猜？）  
   - 要記得「打統編」  
3. 控制時間：1 點前要回來，因為開會

然後你腦中就開始跑出一些副本任務提示：

> 「公司統編多少來著…？」  
> 「一點耶會不會太趕啊…？」  
> 「是不是現在就該出門了…？」

傳統模型在這時候頂多幫你把這句話翻成英文，或幫你列出「To-do 清單」，然後就沒了。  
但有了 MCP 之後，情況就不一樣了，它可以開始**動手幫你跑流程**：

<ul>
  <li>🍱 「拿午餐」：直接連打電話 API，查目前進度，看店家那邊是不是已經可以取餐了</li>
  <li>☕ 「買咖啡」：打開 Google 地圖，找出哪一家咖啡店順路、不用繞</li>
  <li>🧾 「打統編」：連公司資料庫撈出統編，甚至自動幫你填好發票資訊</li>
  <li>🕐 「1 點開會」：跟行事曆同步，幫你設提醒或排除其他行程</li>
</ul>

講白一點：**MCP 就是讓 LLM 有了「手腳」，不只會像我一樣只會講幹話，還真的會動手辦事**。

你只要一句話，它可以幫你拆任務、調度工具、拿出答案，甚至預測你會忘記什麼——然後提早幫你補起來（例如那張一定會漏打的發票）。



### RAG 是什麼？能吃嗎？

是那個傳說中的 Retrieval-Augmented Generation（檢索增強式生成）
簡單講，RAG 就是： ~~大檢索~~

RAG 的流程大概長這樣：

<ol>
  <li>你提供一段問題（Query）</li>
  <li>系統先用 Embedding 向量化這段問題（用像 OpenAI Ada 或自己訓練的 embedding model）</li>
  <li>拿這個向量去 Vector DB（如 FAISS, Pinecone, Weaviate） 做相似度比對，撈出相關資料片段</li>
  <li>把這些「撈回來的資料」塞進 prompt 裡，餵給 LLM</li>
  <li>LLM 再根據這些資料進行回答</li>
</ol>

對比傳統的模型可能會：

<ul>
  <li>胡說八道給你一個看起來合理的答案（但根本亂編）</li>
  <li>或者直接說：「我不知道。」</li>
</ul>

但如果有了 **RAG**，模型就會變得像你那個會幫你 Google、還知道怎麼整理資料的貼心學長姐。

它會這樣處理：

<ol>
  <li>收到問題</li>
  <li>去外部資料庫 / 文件庫 / API 那邊「撈資料」</li>
  <li>把找到的資訊跟原本的知識融合</li>
  <li>給你一個比較不會胡說八道的答案</li>
</ol>

---

你可以想像是這樣的場景：

> 「小季，我剛剛傳給你的報表，那個 2022 年 Q3 的營收小計是多少？」  
>   
> 然後你自己完全不記得。這時候你會怎麼辦？  
>  
> 沒錯，你不是靠腦袋回答，而是：  
> <ol>
>   <li>去開 Google Drive</li>  
>   <li>找出資料夾</li>  
>   <li>打開報表</li>  
>   <li>Ctrl+F「Q3」</li>  
>   <li>找到數字，回覆主管：「23.4 百萬」</li>
> </ol>

恭喜你，你本人就是一個活生生的 RAG。  
你不是靠記憶回答的，是靠 **「現查現用」**。

有了 RAG，你的模型不再是那個「什麼都會講，但講不準」的嘴砲王，  
而是變成一個「先查再說」的負責任大人。

---

就先介紹到這邊啦，其實看這些名詞有時候真的很暈頭轉向和抽象。